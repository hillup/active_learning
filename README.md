# active_learning
主动学习是指通过机器学习的方法获取到那些比较“难”分类的样本数据，让人工再次确认和审核，然后将人工标注得到的数据再次使用有监督学习或者半监督学习模型进行训练，逐步提升模型的效果，将人工经验融入机器学习的模型中。
selection这一步的策略：
- Uncertainty Sampling：将模型中难以区分的样本数据提取出来，提供给业务专家或者标注人员进行标注，从而达到以较快速度提升算法效果的能力
  - Least Confident：选择最大概率最小的样本重新审核
  - Margin Sampling：选择那些极容易被判定为两类的样本数据，或者说这些数据被判定成两类的概率相差不大，即选择模型预测最大和第二大的概率差值最小的样本
  - Entropy：可以用熵来衡量一个系统的不确定性，熵越大表示系统的不确定性越大，熵越小表示系统的不确定性小。因此在二分类或者多分类的场景下，可以选择那些熵比较大的样本数据作为待标注数据
- Query-By-Committee：除了考虑单个模型的不确定性采样方法之外，还可以考虑多个模型的场景，这就是类似集成学习的方法。通过C个模型投票的方式，来选择出那些较“难”区分的样本数据。
  - 投票熵（Vote Entropy）：可以用熵来衡量样本数据被这些分类器区分的难易程度，如果这些分类器都把数据划分到某一类，则容易区分；如果分类器把样本数据划分到多类，则表示难以区分，需要重点关注
  - 平均KL散度（Average KL Divergence）：KL散度可以衡量两个概率之间的“距离”，因此可以用KL散度计算出那些偏差较大的数据样本。
- K-Center Greedy：从待选择的unlabeled pool中，挑选和当前已被选中的样本集s“相对距离最大”的budget个样本（https://arxiv.org/pdf/1708.00489.pdf）
- KMeans Sampling：通过聚类的方法找到“最具代表性”的N个样本加入训练。聚类策略为kmeans方法，首先聚类出N个cluster_center，然后把每个cluster_center匹配到与其距离最近的样本，最终返回N个选择样本

feishu link: https://bytedance.feishu.cn/docs/doccnMTanAiqMQrFBsINtBvaNzc#
